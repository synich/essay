# 编程语言的字符串内部表示

最近在做中文字符校验，结合几种语言的使用，做个总结。除了ASCII字符集以外，其它文字普遍有定义和外部展示的区分，即使不考虑各国定义的标准外，也还存在Unicode和UTF8两种要区分。编程语言接收的输入一定是外部展示，然后在处理时再变成内部表示。

JS
--
因为是内嵌在浏览器，文字的编码方式不需要JS操心，浏览器会把各种编码转成Unicode再给JS。但是JS发明的时候，Unicode还只有BMP，所以内部单元都是UCS2方式，包括String.fromCharCode会截断，比如0x20041返回的是0x41。超过BMP的字符在内部以代理对(surrogate pairs)方式表示，length取得的长度是2。

好在新标准定义了String.fromCodePoint方法能识别代理对，能正确识别0x20041。另外对字符串变量str，用`const i = str[Symbol.iterator]()`得到的i，可以用next()方法每次迭代一个CodePoint，利用这个方法，可以构造另一套支持全Unicode的方法。也算在无奈之下的补偿方式了。

PHP
--
没有语言规范层的定义，实际中可以用`mb_internal_encoding`获取内部编码方式。如果在`mb_`系列方法中编码和输入源不匹配，得到的错误结果要使用者自己承担。有点C语言的哲学。

Python
--
由于出现断代演进，2.x内部是ASCII，3.x内部是Unicode。前者无法支持多语言，后者不适合外部表示，因此2个版本的输入文件都必须指定文件编码方式，否则会以编译时的方式处理，不认识会报错。

2.x的文字只是字节的序列，类型是str(等价于3.x的bytes)。可以加u前缀保存成Unicode，比如u'文字'，类型变成unicode。到了3.x时代，没有unicode类型，str升级成unicode，用bytes表示字节序列。在2.x里经常要对一个str类型变量用decode('utf-8')方法，到了3.x会调用失败，因为此时默认已经是unicode方法，没有decode或encode一说，3.x里只有把str类型变成新增的显示bytes类型，才能使用decode方法。

Golang
--
规范要求输入必须是UTF8。string类型是byte sequence，用`[]`的下标处理时，操作到每个字节。对string用range方法每次返回一个rune类型的值，以Unicode表示的一个字符，长度不定，由于言语出现得比较晚，避免了JS的坑，能表示全部范围。