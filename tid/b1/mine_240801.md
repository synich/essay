# 大模型3实践推理引擎

## 算力简介

大模型对算力要求非常高，神经网络的反向传播的计算量大约是前馈传播的2倍，有时为节约内存，需要激活重计算，这个量和前馈相当，即总flops是前馈的4倍。

赛扬5095，算力不到200Gflops，而手机芯片天玑6020有近500Gflops，手机反而比电脑要快得多。

## 推理引擎

主要作用是提升计算速度和优化硬件资源，包括量化、内存/显存的使用

* olamma: 适合个人使用，它集模型联网管理+lamma.cpp推理引擎于一身
* vLLM/SGLang: 支持多并发，一般企业使用

```
export OLLAMA_HOST=0.0.0.0:11434
ollama serve&
```

启动open-webui，注意-p选项的顺序不要搞错，前面的是主机端口，后面是8080不要变。

```
docker run -d -e HF_ENDPOINT=https://hf-mirror.com -p 3080:8080 -e OLLAMA_BASE_URL=http://192.168.0.224:11434 -v open-webui:/app/backend/data --name open-webui --restart always swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:main
```

## 推理流程的抽象与拆分

### hugging face

将所有的 LLM 抽象成了 text-generation pipeline，由 Model 和 Tokenizer 两部分组成。
tokenizer 需要继承PreTrainedTokenizer类进行实现，该类提供了apply_chat_template方法，可以将多轮对话的 dict 转换为 input id 或者 prompt。也就是经常看到的`<|im_start|>`

### 阿里

提出将llm模型的跨平台部署抽象为4部分

1. tokenizer 处理自然语言输入的关键，将原始文本转换为模型能理解的格式
2. embedding 向量长度在1152~2560不等。为了节约内存，引入了disk embedding
3. blocks 由于LLM的主干网络由一系列连续的block组成。它最主要的两个计算操作分别是Linear（线性化）和MatMul（矩阵乘），除此之外还伴随诸如split、concat和transpose等内存操作，统称为Memory算子
4. lm

### llama3

经过onnx转换得到int4量化文件清单，可以和阿里的抽象对上

1. tokennizer 近13万条长度3~30的二进制串（base64后保存）
2. embedding 近1G的二进制文件
3. block_n 32个113M的文件
4. lm 252M文件

由于block数量很多，全部加载对于小内存设备会不够，ollama的优化策略是开始只加载一个block到内存中，在该block执行推理时，异步加载下一个block；等到加载完毕在执行推理时清空第一个block；这样依次执行，内存中最多有2个block存在，大大降低内存需求。

## 与大模型的交互

### REST协议

由OpenAI定义的JSON格式的HTTP消息格式，以最常用的会话补全为例，认证格式都是Bearer。进行多轮对话要把问答双方的内容都放在messages数组中。如果轮数超出，保留第一个system，删除开始的user消息。

```
curl --request POST \
  --url https://api.siliconflow.cn/v1/chat/completions \
  --header 'Authorization: Bearer sk-arnuqrnsfupcdlyyirqelnvetjocwuqwwcwghymogdvipdql' \
  --header 'Content-Type: application/json' \
  --data '{
  "model": "Qwen/Qwen2.5-7B-Instruct",
  "messages": [
    {
      "content": "You are ChatGPT",
      "role": "system"
    },
    {
      "content": "你会说中文吗",
      "role": "user"
    }
  ],
  "max_tokens": 4096,
  "temperature": 0.8
}'
```

如果响应是SSE格式，收到`[DONE]`后，偶尔浏览器会再发一次POST，并得到关于token的usage。

### langchain和ollama交互

langchain是core，有很多的外围适配。多见的是langchain_llm，和ollama配套的则是langchain_ollama。langchain_core库的主要部分似乎是http库，整体大小只有数M。

两大流派

* ChatOllama(langchain_core.language_models.chat_models.BaseChatModel)
* OllamaLLM(langchain_core.language_models.llms.BaseLLM) 又叫oldfashion

必不可少的embed

* OllamaEmbeddings