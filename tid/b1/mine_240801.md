# 大模型实践

## 算力简介

大模型对算力要求非常高，神经网络的反向传播的计算量大约是前馈传播的2倍，有时为节约内存，需要激活重计算，这个量和前馈相当，即总flops是前馈的4倍。

赛扬5095，算力不到200 Gflops，而手机芯片天玑6020有近500 Gflops，手机反而比电脑要快得多。

## 运行篇-推理引擎

推理引擎的主要作用是利用好硬件计算能力和内存的使用，olamma最普及，它集模型联网管理+lamma.cpp推理引擎于一身，还有vLLM等很多推理引擎。shell启动方式

```
export OLLAMA_HOST=0.0.0.0:11434
ollama serve&
```

启动open-webui，注意-p选项的顺序不要搞错，前面的是主机端口，后面是8080不要变。

```
docker run -d -e HF_ENDPOINT=https://hf-mirror.com -p 3080:8080 -e OLLAMA_BASE_URL=http://192.168.0.224:11434 -v open-webui:/app/backend/data --name open-webui --restart always swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:main
```

langchain和ollama交互

langchain是core，有很多的外围适配。多见的是langchain_llm，和ollama配套的则是langchain_ollama。langchain_core库的主要部分似乎是http库，整体大小只有数M。

两大流派

* ChatOllama(langchain_core.language_models.chat_models.BaseChatModel)
* OllamaLLM(langchain_core.language_models.llms.BaseLLM) 又叫oldfashion

必不可少的embed

* OllamaEmbeddings

## 模型文件

通常LLM模型包含词嵌入+位置编码，并且是神经网络的第一层。通过这两个处理得到的向量数据再进入编码器。向量处理也是模型（或叫embed），结果存入向量数据库。

* model: 核心文件，有架构、参数量、量化方式。arch绝大多数是llama，少量falcon和gemma
* params: 应该是stop词，或者叫tokenizer标志
* template: 可能又叫chat_template

## 流程理解

hugging face 将所有的 LLM 抽象成了 text-generation pipeline，由 Model 和 Tokenizer 两部分组成。
tokenizer 需要继承PreTrainedTokenizer类进行实现，该类提供了apply_chat_template方法，可以将多轮对话的 dict 转换为 input id 或者 prompt。也就是经常看到的`<|im_start|>`

## 交互协议

由OpenAI定义的JSON格式的HTTP消息格式，以最常用的会话补全为例，认证格式都是Bearer。进行多轮对话要把问答双方的内容都放在messages数组中。如果轮数超出，保留第一个system，删除开始的user消息。

```
curl --request POST \
  --url https://api.siliconflow.cn/v1/chat/completions \
  --header 'Authorization: Bearer sk-arnuqrnsfupcdlyyirqelnvetjocwuqwwcwghymogdvipdql' \
  --header 'Content-Type: application/json' \
  --data '{
  "model": "Qwen/Qwen2.5-7B-Instruct",
  "messages": [
    {
      "content": "You are ChatGPT",
      "role": "system"
    },
    {
      "content": "你会说中文吗",
      "role": "user"
    }
  ],
  "max_tokens": 4096,
  "temperature": 0.8
}'
```

如果响应是SSE格式，收到`[DONE]`后，偶尔浏览器会再发一次POST，并得到关于token的usage。