# 大模型实践

## 算力简介

大模型对算力要求非常高，神经网络的反向传播的计算量大约是前馈传播的2倍，有时为节约内存，需要激活重计算，这个量和前馈相当，即总flops是前馈的4倍。

赛扬5095，算力不到200 Gflops，而手机芯片天玑6020有近500 Gflops，手机反而比电脑要快得多。

## 运行篇

shell启动ollama

```
export OLLAMA_HOST=0.0.0.0:11434
ollama serve&
```

启动open-webui，注意-p选项的顺序不要搞错，前面的是主机端口，后面是8080不要变。

```
docker run -d -e HF_ENDPOINT=https://hf-mirror.com -p 3080:8080 -e OLLAMA_BASE_URL=http://192.168.0.224:11434 -v open-webui:/app/backend/data --name open-webui --restart always swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:main
```

langchain和ollama交互

langchain是core，有很多的外围适配。多见的是langchain_llm，和ollama配套的则是langchain_ollama。langchain_core库的主要部分似乎是http库，整体大小只有数M。

## 模型文件

向量处理也是模型（或叫embed），结果存入向量数据库。

* model: 核心文件，有架构、参数量、量化方式。arch绝大多数是llama，少量falcon和gemma
* params: 应该是stop词，或者叫tokenizer标志
* template: 可能又叫chat_template

## 流程理解

hugging face 将所有的 LLM 抽象成了 text-generation pipeline，由 Model 和 Tokenizer 两部分组成。
tokenizer 需要继承PreTrainedTokenizer类进行实现，该类提供了apply_chat_template方法，可以将多轮对话的 dict 转换为 input id 或者 prompt。也就是经常看到的`<|im_start|>`
