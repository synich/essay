# 大模型3实践推理引擎

## 算力简介

大模型对算力要求非常高，神经网络的反向传播的计算量大约是前馈传播的2倍，有时为节约内存，需要激活重计算，这个量和前馈相当，即总flops是前馈的4倍。

赛扬5095，算力不到200Gflops，而手机芯片天玑6020有近500Gflops，手机反而比电脑要快得多。

## 推理引擎

主要作用是提升计算速度和优化硬件资源，包括量化、内存/显存的使用

* olamma: 适合个人使用，它集模型联网管理+lamma.cpp推理引擎于一身
* vLLM/SGLang: 支持多并发，一般企业使用

```
export OLLAMA_HOST=0.0.0.0:11434
ollama serve&
```

启动open-webui，注意-p选项的顺序不要搞错，前面的是主机端口，后面是8080不要变。

```
docker run -d -e HF_ENDPOINT=https://hf-mirror.com -p 3080:8080 -e OLLAMA_BASE_URL=http://192.168.0.224:11434 -v open-webui:/app/backend/data --name open-webui --restart always swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:main
```

## 推理流程的抽象与拆分

### hugging face

将所有的 LLM 抽象成了 text-generation pipeline，由 Model 和 Tokenizer 两部分组成。
tokenizer 需要继承PreTrainedTokenizer类进行实现，该类提供了apply_chat_template方法，可以将多轮对话的 dict 转换为 input id 或者 prompt。也就是经常看到的`<|im_start|>`

### 阿里

提出将llm模型的跨平台部署抽象为4部分

1. tokenizer 处理自然语言输入的关键，将原始文本转换为模型能理解的格式
2. embedding 向量长度在1152~2560不等。为了节约内存，引入了disk embedding
3. blocks 由于LLM的主干网络由一系列连续的block组成。它最主要的两个计算操作分别是Linear（线性化）和MatMul（矩阵乘），除此之外还伴随诸如split、concat和transpose等内存操作，统称为Memory算子
4. lm

### llama3

经过onnx转换得到int4量化文件清单，可以和阿里的抽象对上

1. tokennizer 近13万条长度3~30的二进制串（base64后保存）
2. embedding 近1G的二进制文件
3. block_n 32个113M的文件
4. lm 252M文件

由于block数量很多，全部加载对于小内存设备会不够，ollama的优化策略是开始只加载一个block到内存中，在该block执行推理时，异步加载下一个block；等到加载完毕在执行推理时清空第一个block；这样依次执行，内存中最多有2个block存在，大大降低内存需求。

## 调节推理效果的若干概念

对大模型而言，推理就是随机生成的过程，可以通过以下参数调节生成效果

确定性策略（相比其它概念，这个属于确定类）

* Greedy Search: chat模式几乎都是这个策略，适合流式输出
* Beam Search: 对几个路径进行概率总和评估后，选择最高的输出，适合允许等待且准确性高的场景
* Diverse Beam Search

概率抽样

* Temperature: 0确定贪心 inf完全随机。和logits原始分数强相关
* Top-K抽样
* Top-P(Nucleus) 核抽样
* Rejection 拒绝抽样

惩罚

* Repetition 重复惩罚
* Length 长度惩罚

解码

* Chunk-wise 分块解码
* Staged 分阶段解码
* Contrastive 对比解码
* RL 基于强化学习解码
