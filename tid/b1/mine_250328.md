# 大模型4理解模型文件

## 模型相关概念

模型的权重和激活参数（tensor）是用类似`W4A8`表示，模型权重（Weight）量化到4位，激活值（Active，即模型的输入和输出）量化到8位。模型的主流格式

* pickle: PyTorch原生格式，主要用于调试
* safetensor: 由huggingface设计，侧重安全，但也只能保存tensor。部署需要其它元文件，且不支持量化。被五种框架支持：pytorch、TensorFlow、flax（jax）、paddle（paddlepaddle）、numpy。这些框架训练过程各异，导致各框架的模型格式不一致，无法方便地交换
* gguf: lamma.cpp(ggml)定义。支持打包成一体，支持`Q4_K_M`、`Q3_K_S`等多种量化
* ONNX: 由微软提出，希望能作为一种大一统的模型格式，但实际使用时还是要修改一些源码，比如torch的view要改成squeeze，或是HF的transform要把tuple换成tensor

## 模型的构成

通常LLM模型包含词嵌入+位置编码，并且是神经网络的第一层。通过这两个处理得到的向量数据再进入编码器。向量处理也是模型（或叫embed），结果存入向量数据库。

ollama create新模型需要Modelfile文件，创建后会按PARAMETER/TEMPLATE/SYSTEM归类分组，拆成不同的blobs/sha256文件保存。另外还会生成一个整体描述，部分会体现在/api/tags接口。

* model: 核心文件，有架构、参数量、量化方式。arch有llama、qwen、gemma等多种
* template: 又叫chat_template，必须要有，否则回答无法停止
* params: 指定stop词/temperatue/top_p等参数，有些模型不提供也能正常工作

### 参数规模与量化

流行的大型语言模型（LLM）中，线性层的权重数量常常包含数十亿个参数。比如70亿参数（7b）的模型，采用16位浮点（fp16）存储，需要`7G*2Byte=14G`内存空间。必须采取措施来压缩这些权重，因此有了量化技术。幸运的是，LLM中的线性层权重之间的差异相对较小，这使得它们非常适合进行低比特量化——使用较少的比特表示每个权重值。即使经过量化，计算结果仍能保持与原始浮点计算高度一致，这表明量化对模型性能的影响很小。因此选用Q4_0量化，大小是`7G*0.5Byte=4G`。量化由框架和硬件共同决定，至少有十多种，我所知常见的有fp16/int8/int4。

在LLM领域，训练都是浮点(FP32~FP8)，推理会量化成整数。浮点的指数位比小数位更重要

| 类型 | 符号位 | 指数位 | 精度 |
| ---- | ---- | ---- | ---- |
| FP64 | 1 | 11 | 52 |
| FP32 | 1 | 8 | 23 |
| FP16 | 1 | 5 | 10 |
| BF16 | 1 | 8 | 7 |
| FP8_1 | 1 | 4 | 3 |
| FP8_2 | 1 | 5 | 2 |
| FP4 | 1 | 2 | 1 |

FP8_1(E4M3)用于前向计算，保精度weight和activate，FP8_2(E5M2)用于后向传播，保范围。
类似的BF16相比FP16，BF16牺牲小数位也要保住指数位。BF16的指数位和FP32是一样的。
最极端的FP4能明显看出保指数的意图。
大部分领域对精度要求低，而天文、分子模拟需要高精度会上FP64

### block及其参数

每个block的核心部分是计算Attention和MLP，MLP分为两类：FFN和GLU(Gated Linear Unit)门控单元。主流的文字生成大多使用GLU。每个block有12个或更多参数，这些都被记录在模型modelfile的tensors里。

* Attention参数：QKV各自的bias向量（通常FP32）和weight矩阵（Q3或Q4），加上norm和output。按照理论QKV的weight矩阵shape应该一样，但实际看了qwen2和gemma3后发现，gemma3的视觉类block确实符合理论。但文字block的K和V的weight矩阵shape相同，但Q的weight要比KV大，有说法是Q需要捕捉多样化的注意力模式，而KV用于计算相似度，共享对性能影响小
* MLP参数：视觉类block用mlp.fc1、mlp.fc2（线性层/也叫全连接层）叫法，mlp是multi layer perceptron多层感知机的缩写。传统上以mlp命名，tranformer论文在mlp的基础上加入了激活层后，将这种结构称为FFN，是对全连接的一种扩展。文字block使用了更复杂的门控FFN，有norm,gate,up,down参数。

粗略核算一下模型的参数量：以1.5B的模型为例，`28(block数)x7个(weight数)x2kx4k(平均矩阵形状) = 1.568B`

MoE架构对MLP层进行了优化，可以理解为n个小型FFN(expert)的和。共同知识shared expert，其它routed expert的冗余尽可能少(正交)。Attention参数量较小，但需要存储大量KV缓存(内存密集型)。MLP参数量巨大，MoE的话量更大，但不存储中间结果(计算密集型)