# 大模型2理论篇Attention

## 要解决什么问题

自然语言一直是神经网络的主战场，由于文本的前后相关性，起初都是基于RNN在做，但RNN的串行性导致计算速度快不起来。word2vec在NLP领域使用已久，但每个词向量权重一样，没有考虑上下文的关系。图像领域从12年开始突飞猛进，而文本则迟迟没有大的突破。

2017年Google发表了开创性的论文，提出了Attention机制和Transformer模型（将堆叠的attention和seq2seq结合形成）。Transformer对词的特征在上下文中的特征进行标注（以向量形式表示），辅以Self-Attention/多头attention和Position Embedding来实现对序列数据的建模。Transformer替代RNN来做Seq2Seq任务，使得文本处理得到极大提升。随着论文的发表，发展出了许多Transformer变种，如2018年出现的Bert、以及OpenAI使用的GPT。

## 概念解释

注意力是输入的整个内容与输出的每个词的关系；注意力是个很大的概念，或可以理解为一套编程模式。`attention = softmax(QK^T/dim)xV`，但是不限制QKV怎么来

* Query 相当于原始文件，没有热力图
* Key 要注意的热力图部分，也可以是K和Q通过某种函数得到热力图。总之K都是比较关键的
* Value 注意力在原始文件具现化的部分，是Q的子集

为什么是3个参数？最早起源于信息检索系统，输入条件是Query（红色衬衫），引擎根据Query匹配Key（颜色、品类），根据Query和Key的相似度得到Value（具体商品）。QK求相似度，再V求值。2个参数不够且没法学习权重，4个太多，而第3个可以认为是额外的监督信息，并衍生出很多变体

为什么公式里要有softmax？截止25年6月，有些公司尝试用别的算法部分替代softmax，但仍然要在每8层的最后一层保留softmax，可见非常重要。

### 交叉注意力（cross-attention）

encoder-decoder模型中，decoder采用的就是交叉注意力（注意encoder由于看不到decoder，仍采用self-attention）。Q(encoder)，KV同源(decoder)，Q和KV不同源(cross)。由于encode-decode的使用很少，在此略过

### 自注意力（self-attention）

自注意力顾名思义只来自于encoder或decoder自己。BERT(encoder-only)和GPT(decoder-only)都是自注意力，区别是

* encoder: 双向注意力（bi-direction），或者反过来看，正是因为采用了双向，所以同等条件下的理解能力更强，所以称为编码器
* decoder: 单向注意力(left-right/masked/causal)，注意力关注过去，如果语料足够多，锚点足够靠后，和双向注意力是等效的。加上实现堆叠更简单，所以decoder-only成为了主流

约束条件有两个

1. Q=K=V(同源，由同1个X乘以一个参数矩阵W得来，W是学习得到的)
2. Q,K,V遵循attention的做法

#### 多头注意力（multi-attention）

全称是multi-head self-attention，是自注意力的计算强化版。

每个Q代表一个头(head)，多头注意力机制允许模型同时关注序列中不同位置的信息，从而更好地捕捉序列中的长程依赖关系。多头注意力的head个数在2~128之间，原始论文是8

多头之所有更有效，可能的一个原因是，每个数据代表一个事物，而多头则代表了每个事物不同的属性方面，在求相似度时，由原来的单纯的事物相似，更加细致地演进到属性相似，所以会更精准。比如人，张三跟李四相似，通常会体现很多方面，身高，体重，胖廋，血型，性格。多头注意力会告诉你张三之所以跟李四相似，是来源于身高，或者血型这些很具体的属性方面。

多头虽然提升的效果，但也带来了计算量的膨胀，几种多头的计算效率比较

* MHA: 论文中提出的原始多头，也叫vanila多头，开销最大
* MQA: 之后有人提出MQA，KV全部共用，最节约显存。低参数量时效果尚可
* GQA: 再之后有人担心MQA对KV Cache压缩太严重，又提出GQA。若干个Q成为一组(group)，共享一个KV，是MHA和MQA的折衷。分为8组效果较好
* MLA: 最终经过演化，有了终态MLA（有人认为它是GQA的一般化）可能是效果最好的full attention

### 位置编码

RNN天然理解位置，CNN虽然表面上不支持位置，但有研究猜测卷积核的各项异性使它能分辨出不同方向的相对位置（通过zero padding）。但transformer非常依赖位置编码，向模型提供序列的位置信息，以帮助模型理解序列的顺序信息。

总体分为两种
* 绝对位置编码：将位置信息融入到输入中，原始论文也属于这种，实现简单，速度快
* 相对位置编码：微调模型结构，在算注意力时考虑当前位置与被关注位置的相对距离。性能更好，比较流行的有RoPE

### transformer之脊d_model

输入序列的每个词（token）会被编码为一个维度为`d_model`的向量，后续的注意力层、FFN的输入和输出维度都是d_model，这个维度贯穿整个模型，是最核心的超参数，决定了模型能力和计算效率。

* 微型模型：d_model = 256（文本分类）
* 小型模型：d_model = 512（如原始Transformer）
* 中等模型：d_model = 768（如BERT-base）
* 大型模型：d_model = 1024 或更高（主流LLM）

d_model是多头的总和，原始论文选取8头，每个头维度是512//8=64。FFN的升维或降维都是4*d_model。
