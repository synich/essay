# 数据挖掘2神经网络来源与实现

## 历史

1956年麦卡锡和明斯基在达特茅斯召开了人工智能会议，算是元年，58年罗森布拉特发明了感知机（perceptron），包含有输入层、输出层和一个隐藏层，可以算是神经网络的前身。输入的特征向量通过隐藏层变换到达输出层，由输出层得到分类结果。早期的单层感知机使用线形函数，它对稍微复杂一些的函数都无能为力，明斯基批评感知机没有`异或`能力，否定连接主义，坚持符号派路线。郁郁寡欢的罗森布拉特在1973年死于一次航海。

一层网络（感知机）做不了复杂的事情，多层网络又因为无法解决误差传递问题始终不得其法。直到上世纪八十年代，Hinton、Rumelhart、Werbos等人将梯度下降算法结合反向传播算法，并摆脱早期离散激活函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，使得多层感知机拥有了异或能力，神经网络至此可以被实用化并迎来了大爆发。顺便说一句，梯度下降在1847年就由柯西提出过。

2006年Hinton将连接主义改名为深度学习，并利用预训练的方式缓解了局部最优解的问题，将隐藏层增加到了7层，实现了真正意义上的"深度"。Hinton、LeCun和Bengio一起因深度学习上的贡献获得了2018年图灵奖。辛顿是乔治布尔的曾孙，家中颇多有建树的人。

## 原理介绍和最小实现

神经网络(NN)有三宝：

* 神经元: 最基础的计算单元，比较简单的采用线性计算
* 激活函数和损失函数: 只有线性计算显然不够，于是引入激活函数（具备可导和阶跃特性的非线性函数）和损失函数（判定激活结果和真实结果的误差），二者若不匹配，会导致计算慢甚至错误。Logistic回归可以看成是只有一层的神经网络，同时logistic和激活函数sigmoid是一样的。
* 反向传播: 要得到前面提到的两种计算的参数，就要利用反向传播。它使用激活函数的导数，**也必须用激活函数的导数形式**，因为反向传播的核心是计算损失函数对每个权重的梯度，而激活函数的导数是计算这些梯度的关键。

### 实现最小的神经网络

用numpy实现3层网络（每层都是1个向量）：输入层、隐藏层、输出层。层与层之间通过矩阵的点乘实现变换，这个矩阵称为权重weight。

前向计算使用激活函数，对上述网络来说有两次激活:输入点乘权重后，激活一次；这个输出再点乘权重，再激活一次。激活的结果就是本次输出，通常表示为predicate

反向两次求导，分别是: 误差(actual-predicate) x d/dx(predicate)，隐藏误差 x d/dx隐藏层

神经网络的层数直接决定了它对现实的刻画能力。利用每层更少的神经元拟合更加复杂的函数，但问题出现了，随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，并且这个"陷阱"越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，"梯度消失"现象更加严重。（具体来说，常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。）

softmax是更高维的sigmoid，区别是多分类或二分类，但形式上相似

### 层的分类

神经网络的基本单位是layer，通过相同或不同功能的层的堆叠，实现算法效果

* 全连接层: 又叫dense layer，一般是线性计算。输入与输出完全连接，能够捕捉所有特征关系，具备通用逼近能力，计算量也最大，达到O(n2)。多用于网络的末端。如MLP、分类器输出层
* 卷积层: 仅关注局部特征，比如CNN常用3x3窗口，参数共享
* 循环层: 按时间局部递归连接
* 注意力层: 动态稀疏连接

### 泛化/抗过拟合/广义正则化

机器学习是从数据中学习规律，所以抗过拟合是非常重要的因素，实现数据和模型匹配。从数据、模型和训练过程3个角度有6种方法

* 数据: 从数据的角度，数据增强
* 训练: 有提前终止、权重衰减两种
* 模型: 包括研究损失函数和调整模型结构，有3种
1. 正则化，L1、L2正则化
2. 范数惩罚，和正则化一样，也是研究损失函数
3. Dropout方法，不是研究损失函数，而是调整模型结构丢弃部分数据

以上6种方法都是通过牺牲训练过程中的精度来实现泛化。
