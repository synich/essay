# 数据挖掘2神经网络来源与分类

## 历史

1956年麦卡锡和明斯基在达特茅斯召开了人工智能会议，算是元年，58年罗森布拉特发明了感知机（perceptron），包含有输入层、输出层和一个隐藏层，可以算是神经网络的前身。输入的特征向量通过隐藏层变换到达输出层，由输出层得到分类结果。早期的单层感知机使用线形函数，它对稍微复杂一些的函数都无能为力，明斯基批评感知机没有`异或`能力，否定连接主义，坚持符号派路线。郁郁寡欢的罗森布拉特在1973年死于一次航海。

一层网络（感知机）做不了复杂的事情，多层网络又因为无法解决误差传递问题始终不得其法。直到上世纪八十年代，Hinton、Rumelhart、Werbos等人将梯度下降算法结合反向传播算法，并摆脱早期离散激活函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，使得多层感知机拥有了异或能力，神经网络至此可以被实用化并迎来了大爆发。顺便说一句，梯度下降在1847年就由柯西提出过。

2006年Hinton将连接主义改名为深度学习，并利用预训练的方式缓解了局部最优解的问题，将隐藏层增加到了7层，实现了真正意义上的"深度"。Hinton、LeCun和Bengio一起因深度学习上的贡献获得了2018年图灵奖。辛顿是乔治布尔的曾孙，家中颇多有建树的人。

## 原理介绍和最小实现

神经网络(NN)有三宝：

* 神经元: 最基础的计算单元，比较简单的采用线性计算
* 激活函数和损失函数: 只有线性计算显然不够，于是引入激活函数（具备可导和阶跃特性的非线性函数）和损失函数（判定激活结果和真实结果的误差），二者若不匹配，会导致计算慢甚至错误。Logistic回归可以看成是只有一层的神经网络，同时logistic和激活函数sigmoid是一样的。
* 反向传播: 要得到前面提到的两种计算的参数，就要利用反向传播。它使用激活函数的导数，**也必须用激活函数的导数形式**，因为反向传播的核心是计算损失函数对每个权重的梯度，而激活函数的导数是计算这些梯度的关键。

### 实现最小的神经网络

用numpy实现3层网络（每层都是1个向量）：输入层、隐藏层、输出层。层与层之间通过矩阵的点乘实现变换，这个矩阵称为权重weight。

前向计算使用激活函数，对上述网络来说有两次激活:输入点乘权重后，激活一次；这个输出再点乘权重，再激活一次。激活的结果就是本次输出，通常表示为predicate

反向两次求导，分别是: 误差(actual-predicate) x d/dx(predicate)，隐藏误差 x d/dx隐藏层

神经网络的层数直接决定了它对现实的刻画能力。利用每层更少的神经元拟合更加复杂的函数，但问题出现了，随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，并且这个"陷阱"越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，"梯度消失"现象更加严重。（具体来说，常常使用sigmoid作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。）

softmax是更高维的sigmoid，区别是多分类或二分类，但形式上相似

### 广义正则化方法

有6种：数据增强、提前终止、权重衰减、正则化、范数惩罚、Dropout方法。目的都是在解决过拟合问题，提升泛化能力，也就是实现数据和模型匹配。总体来说，可以从数据、模型和训练过程三个角度理解。

数据增强当然是从数据的角度，提前终止和权重衰减都是从训练过程的角度，L1、L2正则化以及范数惩罚研究的是损失函数，归为模型本身。Dropout方法不是研究损失函数，而是换个角度研究如何调整模型结构。

## 各种网络的简单比较

对神经网络发展变化历程归纳如下：

1. DNN：即多层感知机MLP，也被称为前馈神经网络(Feed-forward Neural Networks)，每层神经元的信号只能向一层传播，样本的处理在各个时刻独立。
2. CNN：图像中存在固有的局部模式（如人脸中的眼睛、鼻子、嘴巴等），所以将图像处理和神将网络结合引出卷积神经网络CNN。CNN是通过卷积核将上下层进行链接，同一个卷积核在所有图像中是共享的，图像通过卷积操作后仍然保留原先的位置关系。
3. RNN：DNN无法对时间序列上的变化进行建模，但时间顺序对于自然语言处理、语音识别等应用非常重要。为了适应这种需求，出现了循环神经网络RNN。

不论哪种网络，在实际应用中常常都混合着使用，比如CNN和RNN在上层输出之前往往会接上全连接层，简单总结CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构的区别：

先说DNN，从结构上来说他和传统意义上的神经网络(NN)没什么区别。一开始的神经元不能表示异或运算，通过增加网络隐藏层数和引入sigmoid激活函数解决了该问题，并发现神经网络的层数直接决定了它对现实的表达能力。但是随着层数的增加会出现局部函数越来越容易出现局部最优解的现象，用数据训练深层网络有时候还不如浅层网络，并会出现梯度消失的问题。为了克服梯度消失，使用ReLU、maxout等激活函数代替了sigmoid，形成了如今DNN的基本形式。

CNN与RNN的比较

相同点

1. 传统神经网络的扩展。
2. 前向计算产生结果，反向计算模型更新。
3. 每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。

不同点

1. CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算
2. RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出
3. CNN可以达到1000+深度，RNN深度有限

在RNN中，神经元的输出可以在下一个时间段直接作用到自身，即第i层神经元在m时刻的输入，除了(i-1)层神经元在该时刻的输出外，还包括其自身在(m-1)时刻的输出。（t+1）时刻网络的最终结果，是该时刻输入和所有历史共同作用的结果，这就达到了对时间序列建模的目的。RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度。

这时"梯度消失"现象又要出现了，只不过这次发生在时间轴上。所以RNN存在无法解决长时依赖的问题。为解决上述问题，提出了LSTM（长短时记忆单元），通过cell门开关实现时间上的记忆功能，并防止梯度消失。在序列信号分析中，如果能预知未来，对识别一定也是有所帮助的。因此就有了双向RNN、双向LSTM，同时利用历史和未来的信息。
