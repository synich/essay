# attention理解

注意力是个很大的概念，或可以理解为一套编程模式。*QKV三个关键参数相乘就是注意力*，但是不限制QKV怎么来，所以衍生出诸如：交叉attention、多头attention、self-attention

* Query 相当于原始文件，没有热力图
* Key 要注意的热力图部分，也可以是K和Q通过某种函数得到热力图。总之K都是比较关键的
* Value 注意力在原始文件具现化的部分，是Q的子集

为什么是3个参数？最早起源于信息检索系统，你的输入条件是Query（红色衬衫），引擎根据Query匹配Key（颜色、品类），根据Query和Key的相似度得到Value（具体商品）。QK求相似度，再V求值。2个参数不够且没法学习权重，4个太多，而第3个可以认为是额外的监督信息。

注意力是输入的整个内容与输出的每个词的关系；自注意力的自，是输入的整个内容，和输入的每个词之间的关系。早期word2vec每个词向量都一样，没有考虑在上下文的关系，所以注意力对词的特征，在上下文中的特征进行标注（以向量形式表示）。

## self-attention

比attention约束条件多两个

1. Q=K=V(同源，由同1个X乘以一个参数矩阵W得来，W是学习得到的)
2. Q,K,V遵循attention的做法

## 交叉注意力

QV不同源，但KV同源

## 影响

注意力是个思想，在这种思想指引下，产生了BERT和GPT。
