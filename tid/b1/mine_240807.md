# 从Attention到GPT

## 要解决什么问题

早期word2vec在NLP领域使用已久，但每个词向量权重一样，没有考虑上下文的关系，而attention对词的特征，在上下文中的特征进行标注（以向量形式表示），使得效果得到极大提升。

## 概念解释

注意力是个很大的概念，或可以理解为一套编程模式。*QKV三个关键参数相乘就是注意力*，但是不限制QKV怎么来，所以衍生出诸如：交叉attention、多头attention、self-attention

* Query 相当于原始文件，没有热力图
* Key 要注意的热力图部分，也可以是K和Q通过某种函数得到热力图。总之K都是比较关键的
* Value 注意力在原始文件具现化的部分，是Q的子集

为什么是3个参数？最早起源于信息检索系统，你的输入条件是Query（红色衬衫），引擎根据Query匹配Key（颜色、品类），根据Query和Key的相似度得到Value（具体商品）。QK求相似度，再V求值。2个参数不够且没法学习权重，4个太多，而第3个可以认为是额外的监督信息。

### self-attention

比attention约束条件多两个

1. Q=K=V(同源，由同1个X乘以一个参数矩阵W得来，W是学习得到的)
2. Q,K,V遵循attention的做法

注意力是输入的整个内容与输出的每个词的关系；自注意力的自，是输入的整个内容，和输入的每个词之间的关系。

### 交叉注意力

KV同源，但Q和KV不同源

## 影响与流变

注意力是个思想，将堆叠的attention和seq2seq结合，产生了Transformer神经网络架构，它的核心思想是通过自注意力机制来实现对序列数据的建模，而不需要使用递归或卷积操作。

Transformer进一步引入多头attention和位置编码。多头注意力机制允许模型同时关注序列中不同位置的信息，从而更好地捕捉序列中的长程依赖关系。位置编码则用于向模型提供关于序列中每个位置的位置信息，以帮助模型理解序列的顺序信息。
之后又发展出了许多变种的Transformer，如BERT、GPT。
