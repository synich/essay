# 从Attention到GPT

## 要解决什么问题

自然语言一直是神经网络的主战场，由于文本的前后相关性，起初都是基于RNN在做，但RNN的串行性导致计算速度快不起来。word2vec在NLP领域使用已久，但每个词向量权重一样，没有考虑上下文的关系。图像领域从12年开始突飞猛进，而文本则迟迟没有大的突破。

2017年Google发表了开创性的论文，提出了Attention机制和Transformer模型（将堆叠的attention和seq2seq结合形成）。Transformer对词的特征在上下文中的特征进行标注（以向量形式表示），辅以Self-Attention/多头attention和Position Embedding来实现对序列数据的建模。Transformer替代RNN来做Seq2Seq任务，使得文本处理得到极大提升。

## 概念解释

注意力是个很大的概念，或可以理解为一套编程模式。*QKV三个关键参数相乘就是注意力*，但是不限制QKV怎么来，所以衍生出诸如：交叉attention、多头attention、self-attention

* Query 相当于原始文件，没有热力图
* Key 要注意的热力图部分，也可以是K和Q通过某种函数得到热力图。总之K都是比较关键的
* Value 注意力在原始文件具现化的部分，是Q的子集

为什么是3个参数？最早起源于信息检索系统，你的输入条件是Query（红色衬衫），引擎根据Query匹配Key（颜色、品类），根据Query和Key的相似度得到Value（具体商品）。QK求相似度，再V求值。2个参数不够且没法学习权重，4个太多，而第3个可以认为是额外的监督信息。

### self-attention

比attention约束条件多两个

1. Q=K=V(同源，由同1个X乘以一个参数矩阵W得来，W是学习得到的)
2. Q,K,V遵循attention的做法

注意力是输入的整个内容与输出的每个词的关系；自注意力的自，是输入的整个内容，和输入的每个词之间的关系。

### 交叉注意力

KV同源，但Q和KV不同源

### 多头注意力

多头注意力机制允许模型同时关注序列中不同位置的信息，从而更好地捕捉序列中的长程依赖关系。多头之所有更有效，可能的一个原因是，每个数据代表一个事物，而多头则代表了每个事物不同的属性方面，在求相似度时，由原来的单纯的事物相似，更加细致地演进到属性相似，所以会更精准。比如人，张三跟李四相似，通常会体现很多方面，身高，体重，胖廋，血型，性格。多头注意力会告诉你张三之所以跟李四相似，是来源于身高，或者血型这些很具体的属性方面。

位置编码则用于向模型提供关于序列中每个位置的位置信息，以帮助模型理解序列的顺序信息。

## 影响与流变

随着论文的发表，发展出了许多Transformer变种，如2018年出现的Bert、以及OpenAI使用的GPT。
