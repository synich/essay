# 大模型6性能优化

## 性能瓶颈

大Prompt结合多并发对性能影响极大。同样的硬件，4B的5并发+4K prompt速度还略差于14B的单并发+0.5K Prompt，所以对低端硬件来说缩短Prompt是有价值的。

## 训练优化

LoRa模式对训练组非常友好，不同目的的语料分开训练，互相不会干扰。LoRa的秩要达到64比较可用，部分场景甚至要128后准确率才有保障，导致训出来的LoRa文件体积达到原始文件的1/20~1/10，以至于使用LoRa后性能拖慢10%～25%。加上MindIE不支持LoRa和prefix cache同时打开，在多并发场景时性能更是雪上加霜。

堆积语料的训练效果非常明显，似乎数量达到3万条左右，哪怕4B的模型都能超过14B的效果，而且训练4B不到6小时。这对垂直领域来说是非常适合的。

## 推理优化

对MindIE来说，对模型权重(safetensor)做组图有python和cpp两种模式：理论上cpp专为部署优化，性能更好，实际单并发下速度没有更快，只是多了prefix cache功能，但就是这个看似不起眼的功能，在5并发2K的prompt时，可以把速度从3.5提升到7 token/s。python组图将safetensor导入显存这步实现得更好，所以省内存。似乎nVidia上没有这些问题，可见推理框架要做成熟的技术难度很高。

KV Cache对推理很重要，没有LoRa的时候，将模型加载后的剩余内存都给KV Cache，工作得很好。使用LoRa后再用此策略会导致LoRa计算时无法分配到足够大的显存，只能采用固定大小方式留给KV Cache。因为LoRa本就不能共享KV Cache，所以显存留得少也没有大影响。

组图和融合是偏上层的优化，真正的矩阵加速还是要靠更偏硬件的库，对于MindIE来说就是ATB库和Ascend CL驱动，不过这块并不开源，无从得知究竟。

推理最后一个环节是计算出logits，然后从vocab采样选一个token，temperature/top-p等参数影响的就是这步。据说这步是基于CPU做的，所以参数对性能没有影响。

## 源码

MindIE是cpp与python混合项目，入口`mindieservice_daemon`是cpp程序，启动流程如下

```
main() -> Endpoint::Start()
 |_ grpc_handle -> decode
 |_ tokenize_pool
 |_ http_server -> http_handle -> prefill
 |_ Engine::Init
   |_execvp(python进程)
```

推理过程

```
Server -> Engine
 -> Scheduler(是否batch，决定queue_wait时间)
  -> BlockManager(分配KV Cache、复用prefix Cache)
   -> executor(调用CANN)
```

从Scheduler到executor会循环多次，每次称为一次decode