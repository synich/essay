# 数据挖掘1传统机器学习分类

虽然ML已经日渐式微，但了解其思想才能更好地理解现在的方向。机器学习的目的有三大类：回归、分类、聚类。回归和分类都是概率模型，而聚类则基于距离，与概率论无关。再辅以配套技术：降维、模型选择和预处理。

## 基础概念

* 损失函数: 又叫目标函数，对应自动化控制中的误差，因为模型的拟合最终是取决于总偏差值

大多数监督学习模型（分类、回归）和显式概率模型（如自回归模型）直接基于最大似然估计（MLE），覆盖了核心应用。而生成模型（如GAN）、对比学习、部分强化学习方法和贝叶斯模型采用了非最大似然估计（MLE）的目标函数或框架。

## 有监督-回归（线性回归逻辑回归）

## 有监督-分类

* KNN决策树
* 朴素贝叶斯
* 支持向量机（SVM）

SVM值得展开说说，它始于 *最大间隔* ，在 *高维映射* 迎来升华，最后通过 *核方法* 修成正果。

## 无监督-聚类K-means

## 集成学习

将多个弱模型整合成一个强模型，有bagging和boosting两个流派

## bagging派之随机森林

## boosting派

* XGBoost: 最有名
* AdaBoost
* GBDT

## 强化学习

在人工智能领域属于少见的行为主义学派（控制论）。实现时与神经网络关联很深，似乎只有AlphaGo成功了，出了游戏领域没有成功的例子
