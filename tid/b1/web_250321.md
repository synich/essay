# Web的音频传输

既有对讲属性又有网络打洞过程，网络方面要同时有TCP和UDP，协议非常复杂。这两件事都在交换SDP时完成

* 媒体格式协商：涉及音视频参数、使用端口，体现在m=application、a=group
* 必须有ICE过程：可以没有STUN或TURN，即使在直通网络也要ICE。体现在a=candidate、a=sctp-port

ICE 候选有以下几种

* host：本地网络接口的候选
* srflx：通过 STUN 服务器获取的公网候选
* relay：通过 TURN 服务器获取的中继候选

## WebRTC

js和py(aiortc)两端的API都基于RTCPeerConnection，有createOffer/createAnswer/setLocalDescription/setRemoteDescription，通过交换SDP完成协商，开始媒体流传输

```
js client -> SDP(type offer)
  .createDataChannel  // 只支持发送 ArrayBuffer，如果是Blob对象，要通过FileReader.readAsArrayBuffer转换
  .createOffer
  .setLocalDescription
  HTTP发送Local SDP给服务端
  接收answer并.setRemoteDescription
py server -> SDP(type answer)
```

SDP的JSON格式： {"sdp":"", "type":"offer/answer"}，其中sdp是多行文本

```
v=0
o=- 3951366850 3951366850 IN IP4 0.0.0.0
s=-
t=0 0
m=application 43400 DTLS/SCTP 5000
c=IN IP4 10.32.12.2
a=group:BUNDLE 0
a=msid-semantic:WMS *
```

查错使用独立的页面： chrome://webrtc-internals/

### JS的音频采集

首先获取音源（原始音源，未编码），且必须在HTTPS或localhost环境

`const stream = await navigator.mediaDevices.getUserMedia({ audio: true });`

然后通过MediaRecorder配置mimeType和采样间隔，MediaRecorder默认支持WebM和MP4，但不支持WAV。因为前者适合流式传输场景，后者不行。

```
new MediaRecorder(stream, { mimeType: 'audio/webm' });
mediaRecorder.start(600); // 每600ms一次音频编码
mediaRecorder.ondataavailable = (event) => {} // event.data Blob类型
```

## HTTP/2

由于WebRTC过于复杂，采用fetch+MediaRecorder简化过程。H2属于Python的ASGI框架，所以不能用Flask，要换成quart+hypercorn

JS使用 TransformStream + fetch失败，退回 Blob + FormData方式
