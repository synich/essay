# 大模型1历史流变

23年12月意外地被拉进大模型项目后大约跟了3个月，不得已学着理解这其中一些概念，一些不成熟的思考

## 传统NLP任务

通常分为三类:

* NLU(文本分类,分词,句法分析,信息抽取等)
* 有条件生成任务(seq-seq,如翻译任务,QA)
* 无条件生成任务(用预训练模型直接生成内容)

预训练模型也分为三类，分别在前面三种任务中表现良好:

* 编码解码 T5
* 自编码 BERT
* 自回归 GPT

语言理解,计算机科学把 nlp 分为 nlu 和 nlg,我们大脑的皮层有布洛卡区和韦尼克区,布洛卡区对应的是 nlg 而韦尼克区对应的 nlu

婴儿在学习语言的时候,是把声音的刺激和实物还有他的感受融合在一起,所以在这种情况下,如果是双语并行刺激,形成的布洛卡区是一个区域.但是等他长到了小学以后,他在已经有唯一母语的情况下去学习其他外语,是用文字或者声音的方式去和母语建立关联,这种情况下,布洛卡区是两个挨得很近,但独立的区域 

## 训练和推理

用户代码  -> AI框架（PyTorch/Tensorflow/Caffe等）-> CUDA lib -> Driver -> 显卡

* 训练过程： 前向传播  -> 后向传播 -> 梯度更新（迭代重复）
* 推理过程： 前向传播 （迭代重复）

推理只是完成了训练的一部分内容，当然推理还可以剪枝、压缩让前向传播更快。但总体而言完成一次迭代（同batch_size），训练需要的运算量更多，但前向传播的底层运算基本相同。

tensor core和cuda core 都是运算单元，是硬件名词，其主要的差异是算力和运算场景。场景：cuda core是全能通吃型的浮点运算单元，tensor core专门为深度学习矩阵运算设计。算力：在高精度矩阵运算上 tensor cores吊打cuda cores。

2010年英伟达发布的Fermi架构，是第一个完整的GPU架构。其计算核心由16个SM（Stream Multiprocesser）组成，每个SM包含2个线程束（Warp），16组加载存储单元（LD/ST）和4个特殊函数单元（SFU）组成。最核心的是，每个线程束包含16个Cuda Core组成，每一个Cuda Core包含了一个整数运算单元integer arithmetic logic unit (ALU) 和一个浮点运算单元floating point unit (FPU)。然后，这个core能进行一种fused multiply-add (FMA)的操作，通俗一点就是一个加乘操作的融合。特点：在不掉精度的情况下，单指令完成乘加操作，并且这个是支持32-bit精度。更通俗一点，就是深度学习里面的操作变快了。

Turing架构Tensor核心中设计添加了INT8和INT4精度模式，以推断可以容忍量化的工作负载。而Ampere架构GA10x GPU中的新第三代Tensor Core架构可加速更多数据类型，并包括新的稀疏性功能。

使用Tensor核的两个CUDA库是cuBLAS和cuDNN。cuBLAS使用张量核加速GEMM计算（GEMM是矩阵-矩阵乘法的BLAS术语）；cuDNN使用张量核加速卷积和递归神经网络（RNNs）。

训练和推理有各自的专用框架，训练重在并行（张量并行/数量并行），推理重在压缩（剪枝/量化）。

LLM推理可以被分为两个阶段：prefill(context)和decode(generate)。在prefill阶段，输入一个包含m个token的提示（prompt），执行batch为m的推理，此时由于是初始输入，没有kv-cache的需求，得到了首个token的输出。接着，在decode阶段，以上一轮的输出token作为输入，进行batch的推理，这时kv-cache的长度变为m+n，其中n代表之前已生成的token数量。单个block的耗时分析显示：

1. prefill阶段，Linear算子的耗时占比相对稳定，超过了93%，而MatMul和Memory算子的耗时占比分别约为3%和2%；这个阶段处理大量输入数据，使用矩阵乘法（GEMM），是计算密集过程，需要用SIMD优化
2. decode阶段，随着m+n的增长，Linear算子的时间占比有所下降，而MatMul和Memory算子的占比有所上升。尽管如此，在多数情况下，耗时主要集中在Linear算子上。这个阶段使用矩阵向量乘法（GEMV），访存的效率变得更加关键
