# 对大模型与的理解与思考

23年12月意外地被拉进大模型项目后大约跟了3个月，不得已学着理解这其中一些概念，一些不成熟的思考

## 传统NLP任务

通常分为三类:

* NLU(文本分类,分词,句法分析,信息抽取等)
* 有条件生成任务(seq-seq,如翻译任务,QA)
* 无条件生成任务(用预训练模型直接生成内容)

预训练模型也分为三类,分别在前面三种任务中表现良好.分别是:

* 编码解码 T5
* 自编码 BERT
* 自回归 GPT

语言理解,计算机科学把 nlp 分为 nlu 和 nlg,我们大脑的皮层有布洛卡区和韦尼克区,布洛卡区对应的是 nlg 而韦尼克区对应的 nlu

婴儿在学习语言的时候,是把声音的刺激和实物还有他的感受融合在一起,所以在这种情况下,如果是双语并行刺激,形成的布洛卡区是一个区域.但是等他长到了小学以后,他在已经有唯一母语的情况下去学习其他外语,是用文字或者声音的方式去和母语建立关联,这种情况下,布洛卡区是两个挨得很近,但独立的区域 

## 模型相关概念

早期模型的权重和激活参数（tensor）是用PyTorch的pickle保存，比如`W4A8`方案就表示模型权重（Weight）量化到4位，激活值（Active，即模型的输入和输出）量化到8位。现在主流的有huggingface设计的safetensor和ggml定义的gguf。

### 模型格式

safetensor支持五种框架：pytorch、TensorFlow、flax（jax）、paddle（paddlepaddle）、numpy。这些框架训练过程各异，导致各框架的模型格式不一致，无法方便地交换。于是提出了ONNX格式希望能作为一种大一统的模型格式，但实际使用时还是要修改一些源码，比如torch的view要改成squeeze，或是HF的transform要把tuple换成tensor。

阿里提出将llm模型的跨平台部署抽象为4部分

1. tokenizer 处理自然语言输入的关键，将原始文本转换为模型能理解的格式
2. embedding 为了节约内存，引入了disk embedding
3. blocks 由于LLM的主干网络由一系列连续的block组成，每个block的核心计算部分是Attention，它最主要的两个计算操作分别是Linear（线性化）和MatMul（矩阵乘），除此之外还伴随诸如split、concat和transpose等内存操作，统称为Memory算子
4. lm

将llama3-8经过onnx转换得到int4量化文件清单如下

1. tokennizer 近13万条长度3~30的二进制串（base64后保存）
2. embedding 近1G的二进制文件
3. block_n 32个113M的文件
4. lm 252M文件

### 规模与量化

流行的大型语言模型（LLM）中，线性层的权重数量常常包含数十亿个参数。比如70亿参数（7b）的模型，采用16位浮点（fp16）存储，需要`7G*2Byte=14G`内存空间。必须采取措施来压缩这些权重，因此有了量化技术。幸运的是，LLM中的线性层权重之间的差异相对较小，这使得它们非常适合进行低比特量化——即使用较少的比特表示每个权重值。即使经过量化，计算结果仍能保持与原始浮点计算高度一致，这表明量化对模型性能的影响很小。因此选用Q4_0量化，大小是`7G*0.5Byte=4G`。量化由框架和硬件共同决定，至少有十多种，我所知常见的有fp16/int8/int4。

模型的核心结构是张量，我下载过1B的模型文件只有200个张量，平均每个张量的参数有500万之多。

### 训练和推理

用户代码  -> AI框架（PyTorch/Tensorflow/Caffe等）-> CUDA lib -> Driver -> 显卡

* 训练过程： 前向传播  -> 后向传播 -> 梯度更新。（迭代重复）
* 推理过程： 前向传播 。 （迭代重复）

推理只是完成了训练的一部分内容，当然推理还可以剪枝、压缩让前向传播更快。但总体而言完成一次迭代（同batch_size），训练需要的运算量更多，但前向传播的底层运算基本相同。

tensor core和cuda core 都是运算单元，是硬件名词，其主要的差异是算力和运算场景。场景：cuda core是全能通吃型的浮点运算单元，tensor core专门为深度学习矩阵运算设计。算力：在高精度矩阵运算上 tensor cores吊打cuda cores。

2010年英伟达发布的Fermi架构，是第一个完整的GPU架构。其计算核心由16个SM（Stream Multiprocesser）组成，每个SM包含2个线程束（Warp），16组加载存储单元（LD/ST）和4个特殊函数单元（SFU）组成。最核心的是，每个线程束包含16个Cuda Core组成，每一个Cuda Core包含了一个整数运算单元integer arithmetic logic unit (ALU) 和一个浮点运算单元floating point unit (FPU)。然后，这个core能进行一种fused multiply-add (FMA)的操作，通俗一点就是一个加乘操作的融合。特点：在不掉精度的情况下，单指令完成乘加操作，并且这个是支持32-bit精度。更通俗一点，就是深度学习里面的操作变快了。

Turing架构Tensor核心中设计添加了INT8和INT4精度模式，以推断可以容忍量化的工作负载。而Ampere架构GA10x GPU中的新第三代Tensor Core架构可加速更多数据类型，并包括新的稀疏性功能。

使用Tensor核的两个CUDA库是cuBLAS和cuDNN。cuBLAS使用张量核加速GEMM计算（GEMM是矩阵-矩阵乘法的BLAS术语）；cuDNN使用张量核加速卷积和递归神经网络（RNNs）。

训练和推理有各自的专用框架，训练重在并行（张量并行/数量并行），推理重在压缩（剪枝/量化）。

LLM推理可以被分为两个阶段：prefill(context)和decode(generate)。在prefill阶段，输入一个包含m个token的提示（prompt），执行batch为m的推理，此时由于是初始输入，没有kv-cache的需求，得到了首个token的输出。接着，在decode阶段，以上一轮的输出token作为输入，进行batch的推理，这时kv-cache的长度变为m+n，其中n代表之前已生成的token数量。单个block的耗时分析显示：

1. prefill阶段，Linear算子的耗时占比相对稳定，超过了93%，而MatMul和Memory算子的耗时占比分别约为3%和2%；这个阶段处理大量输入数据，使用矩阵乘法（GEMM），是计算密集过程，需要用SIMD优化
2. decode阶段，随着m+n的增长，Linear算子的时间占比有所下降，而MatMul和Memory算子的占比有所上升。尽管如此，在多数情况下，耗时主要集中在Linear算子上。这个阶段使用矩阵向量乘法（GEMV），访存的效率变得更加关键
