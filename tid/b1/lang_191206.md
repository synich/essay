# 编程语言的字符串内部表示

最近在做中文字符校验，结合几种语言的使用，做个总结。除了ASCII字符集以外，其它文字普遍有定义和外部展示的区分，即使不考虑各国定义的标准外，也还存在Unicode和UTF8两种要区分。编程语言接收的输入一定是外部展示，然后在处理时再变成内部表示。

## JS

因为是内嵌在浏览器，文字的编码方式不需要JS操心，浏览器会把各种编码转成Unicode再给JS。但是JS发明的时候，Unicode还只有BMP，所以内部单元都是UCS2方式，包括String.fromCharCode会截断，比如0x20041返回的是0x41。超过BMP的字符在内部以代理对(surrogate pairs)方式表示，length取得的长度是2。

好在新标准定义了String.fromCodePoint方法能识别代理对，能正确识别0x20041。另外对字符串变量str，用`const i = str[Symbol.iterator]()`得到的i，可以用next()方法每次迭代一个CodePoint，利用这个方法，可以构造另一套支持全Unicode的方法。也算在无奈之下的补偿方式了。

## PHP

没有语言规范层的定义，实际中可以用`mb_internal_encoding`获取内部编码方式。如果在`mb_`系列方法中编码和输入源不匹配，得到的错误结果要使用者自己承担。有点C语言的哲学。

## Python

由于出现断代变迁，2和3有较大差异。2.x内部是ASCII，3.x内部是Unicode。前者无法支持多语言，后者不是通用的外部表示（因为主流是UTF-8，在那之前则是各国不同的编码标准），因此2个版本的输入文件都有文件编码参数（可以显式指定，或跟随操作系统），如果读入的字符和指定的参数有冲突会报错。

2.x的文字只是字节的序列，类型是str(等价于3.x的bytes)。可以加u前缀保存成Unicode，比如u'文字'，类型变成unicode。到了3.x时代，str升级成unicode，bytes表示字节序列。在2.x里经常要对一个str类型变量用decode("utf-8")方法，到了3.x会调用失败，因为str已经是unicode类型，只有encode成某种编码的序列；反之byte类型才有decode方法，将一段字节流按指定的格式解码成unicode形式。

python的base64解码，由于返回值不能保证内容是unicode可编码，所以只能是bytes类型。如果想要以str方式使用，要decode("utf-8")后再使用。

## Golang

规范要求输入必须是UTF8。string类型是byte sequence，用`[]`的下标处理时，操作到每个字节。对string用range方法每次返回一个rune类型的值，以Unicode表示的一个字符，长度不定，由于语言出现得比较晚，避免了JS的坑，能表示全部范围。