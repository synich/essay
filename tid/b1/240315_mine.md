# 对大模型与的理解与思考

23年12月意外地被拉进大模型项目后大约跟了3个月，不得已学着理解这其中一些概念，一些不成熟的思考

## 传统NLP任务

通常分为三类:

* NLU(文本分类,分词,句法分析,信息抽取等)
* 有条件生成任务(seq-seq,如翻译任务,QA)
* 无条件生成任务(用预训练模型直接生成内容)

预训练模型也分为三类,分别在前面三种任务中表现良好.分别是:

* 编码解码 T5
* 自编码 BERT
* 自回归 GPT

语言理解,计算机科学把 nlp 分为 nlu 和 nlg,我们大脑的皮层有布洛卡区和韦尼克区,布洛卡区对应的是 nlg 而韦尼克区对应的 nlu

婴儿在学习语言的时候,是把声音的刺激和实物还有他的感受融合在一起,所以在这种情况下,如果是双语并行刺激,形成的布洛卡区是一个区域.但是等他长到了小学以后,他在已经有唯一母语的情况下去学习其他外语,是用文字或者声音的方式去和母语建立关联,这种情况下,布洛卡区是两个挨得很近,但独立的区域 

## 模型相关概念

早期模型的权重和激活参数（tensor）是用PyTorch的pickle保存，现在主流的有huggingface设计的safetensor和ggml定义的gguf。

safetensor支持五种框架：pytorch、TensorFlow、flax（jax）、paddle（paddlepaddle）、numpy。

模型规模与量化

10B以下通常称为小模型，再结合量化方式就可以确定使用资源，比如7B模型如果用BF16，大小是`7G*2Byte=14G`；如果用Q4_0量化，大小是`7G*0.5Byte=4G`。量化由框架和硬件共同决定，至少有十多种，我所知常见的有fp16/int8/int4。

模型的核心结构是张量，我下载过1B的模型文件只有200个张量，平均每个张量的参数有500万之多。

### 训练和推理

用户代码  -> AI框架（PyTorch/Tensorflow/Caffe等）-> CUDA lib -> Driver -> 显卡

* 训练过程： 前向传播  -> 后向传播 -> 梯度更新。（迭代重复）
* 推理过程： 前向传播 。 （迭代重复）

推理只是完成了训练的一部分内容，当然推理还可以剪枝、压缩让前向传播更快。但总体而言完成一次迭代（同batch_size），训练需要的运算量更多，但前向传播的底层运算基本相同。

tensor core和cuda core 都是运算单元，是硬件名词，其主要的差异是算力和运算场景。场景：cuda core是全能通吃型的浮点运算单元，tensor core专门为深度学习矩阵运算设计。算力：在高精度矩阵运算上 tensor cores吊打cuda cores。

2010年英伟达发布的Fermi架构，是第一个完整的GPU架构。其计算核心由16个SM（Stream Multiprocesser）组成，每个SM包含2个线程束（Warp），16组加载存储单元（LD/ST）和4个特殊函数单元（SFU）组成。最核心的是，每个线程束包含16个Cuda Core组成，每一个Cuda Core包含了一个整数运算单元integer arithmetic logic unit (ALU) 和一个浮点运算单元floating point unit (FPU)。然后，这个core能进行一种fused multiply-add (FMA)的操作，通俗一点就是一个加乘操作的融合。特点：在不掉精度的情况下，单指令完成乘加操作，并且这个是支持32-bit精度。更通俗一点，就是深度学习里面的操作变快了。

Turing架构Tensor核心中设计添加了INT8和INT4精度模式，以推断可以容忍量化的工作负载。而Ampere架构GA10x GPU中的新第三代Tensor Core架构可加速更多数据类型，并包括新的稀疏性功能。

使用Tensor核的两个CUDA库是cuBLAS和cuDNN。cuBLAS使用张量核加速GEMM计算（GEMM是矩阵-矩阵乘法的BLAS术语）；cuDNN使用张量核加速卷积和递归神经网络（RNNs）。

## 传统机器学习分类概览

虽然ML已经日渐式微，但了解其思想才能更好地理解现在的方向。机器学习的目的有三大类：回归、分类、聚类。再辅以配套技术：降维、模型选择和预处理。

## 有监督-回归线性回归逻辑回归

## 有监督-分类

* KNN决策树
* 支持向量机
* SVM和朴素贝叶斯

## 无监督-聚类K-means

# 集成学习

将多个弱模型整合成一个强模型，有bagging和boosting两个流派

## bagging派之随机森林

## boosting派之XGBoost其实还有AdaBoost和GBDT，但XGBoost最有名

# 强化学习

在人工智能领域属于少见的行为主义学派（控制论）。实现时与神经网络关联很深，似乎只有AlphaGo成功了，似乎出了游戏领域没有成功的例子